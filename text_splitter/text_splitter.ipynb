{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It is a very important pre processing technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_length_chunking(text, chunk_size):\n",
    "    words = text.split() \n",
    "    chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "TEXT = \"Machine learning is a field of AI that enables computers to learn from data. Deep learning is a subset of machine learning focused on neural networks. These techniques have revolutionized industries like healthcare, finance, and robotics.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk\n",
    "\n",
    "chunk_size=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character=a,b,c,d,e,f,g,h,i,j,k\n",
    "token(word)=\"sunny\"\n",
    "sent\n",
    "para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = TEXT.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine',\n",
       " 'learning',\n",
       " 'is',\n",
       " 'a',\n",
       " 'field',\n",
       " 'of',\n",
       " 'AI',\n",
       " 'that',\n",
       " 'enables',\n",
       " 'computers',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'from',\n",
       " 'data.',\n",
       " 'Deep',\n",
       " 'learning',\n",
       " 'is',\n",
       " 'a',\n",
       " 'subset',\n",
       " 'of',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'focused',\n",
       " 'on',\n",
       " 'neural',\n",
       " 'networks.',\n",
       " 'These',\n",
       " 'techniques',\n",
       " 'have',\n",
       " 'revolutionized',\n",
       " 'industries',\n",
       " 'like',\n",
       " 'healthcare,',\n",
       " 'finance,',\n",
       " 'and',\n",
       " 'robotics.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [' '.join(words[i:i + CHUNK_SIZE]) for i in range(0, len(words), CHUNK_SIZE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0->36\n",
    "0,10,20,30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(words), CHUNK_SIZE):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine',\n",
       " 'learning',\n",
       " 'is',\n",
       " 'a',\n",
       " 'field',\n",
       " 'of',\n",
       " 'AI',\n",
       " 'that',\n",
       " 'enables',\n",
       " 'computers']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0:0 + CHUNK_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'industries like healthcare, finance, and robotics.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(words[i:i + CHUNK_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to', 'learn', 'from', 'data.', 'Deep', 'learning', 'is', 'a', 'subset', 'of']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[10:10 + CHUNK_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(words), CHUNK_SIZE):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = fixed_length_chunking(text=TEXT, chunk_size=CHUNK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning is a field of AI that enables computers to learn from data. Deep learning is a subset of machine learning focused on neural networks. These techniques have revolutionized industries like healthcare, finance, and robotics.\n"
     ]
    }
   ],
   "source": [
    "print(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: Machine learning is a field of AI that enables computers to learn from data. Deep learning is a subset of\n",
      "\n",
      "Chunk 2: machine learning focused on neural networks. These techniques have revolutionized industries like healthcare, finance, and robotics.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking with Character text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\n",
    "Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\n",
    "Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\n",
    "Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=50, chunk_overlap=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(separator=\" \", chunk_size=50, chunk_overlap=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_text(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: Lorem ipsum dolor sit amet, consectetur\n",
      "\n",
      "Chunk 2: adipiscing elit. Sed do eiusmod tempor incididunt\n",
      "\n",
      "Chunk 3: incididunt ut labore et dolore magna aliqua.\n",
      "Ut\n",
      "\n",
      "Chunk 4: aliqua.\n",
      "Ut enim ad minim veniam, quis nostrud\n",
      "\n",
      "Chunk 5: nostrud exercitation ullamco laboris nisi ut\n",
      "\n",
      "Chunk 6: nisi ut aliquip ex ea commodo consequat.\n",
      "Duis aute\n",
      "\n",
      "Chunk 7: aute irure dolor in reprehenderit in voluptate\n",
      "\n",
      "Chunk 8: voluptate velit esse cillum dolore eu fugiat nulla\n",
      "\n",
      "Chunk 9: nulla pariatur.\n",
      "Excepteur sint occaecat cupidatat\n",
      "\n",
      "Chunk 10: cupidatat non proident, sunt in culpa qui officia\n",
      "\n",
      "Chunk 11: officia deserunt mollit anim id est laborum.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load example document\n",
    "with open(\"C:\\\\Complete_Content\\\\GENERATIVEAI\\\\NEW_E2E_COURSE\\\\genai_bootcamp\\\\data\\\\state_of_the_union.txt\") as f:\n",
    "    state_of_the_union = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.create_documents([state_of_the_union])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and'\n",
      "page_content='of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.'\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100\n",
    "\n",
    "100\n",
    "150\n",
    "200\n",
    "250\n",
    "2100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
